{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data  # features\n",
    "y = data.target  # target labels (0 for malignant, 1 for benign)\n",
    "\n",
    "# Optionally, display the description - uncomment the following line to do so\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    z = np.clip( x, -500, 500 )           # protect against overflow\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic(X, y, w, b, lambda_=0, safe=False):\n",
    "    \"\"\"\n",
    "    Computes cost using logistic loss, non-matrix version\n",
    "\n",
    "    Args:\n",
    "      X (ndarray): Shape (m,n)  matrix of examples with n features\n",
    "      y (ndarray): Shape (m,)   target values\n",
    "      w (ndarray): Shape (n,)   parameters for prediction\n",
    "      b (scalar):               parameter  for prediction\n",
    "      lambda_ : (scalar, float) Controls amount of regularization, 0 = no regularization\n",
    "      safe : (boolean)          True-selects under/overflow safe algorithm\n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m,n = X.shape\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        z_i    = np.dot(X[i],w) + b                                             #(n,)(n,) or (n,) ()\n",
    "        if safe:  #avoids overflows\n",
    "            cost += -(y[i] * z_i ) + log_1pexp(z_i)\n",
    "        else:\n",
    "            f_wb_i = sigmoid(z_i)                                                   #(n,)\n",
    "            # Added small value to avoid log(0)\n",
    "            cost += (-y[i] * np.log(f_wb_i + 1e-10) - (1 - y[i]) * np.log(1 - f_wb_i + 1e-10)) + (lambda_/(2*m))*np.dot(w,w)\n",
    "    cost = cost/m\n",
    "\n",
    "    reg_cost = 0\n",
    "    if lambda_ != 0:\n",
    "        for j in range(n):\n",
    "            reg_cost += (w[j]**2)                                               # scalar\n",
    "        reg_cost = (lambda_/(2*m))*reg_cost\n",
    "\n",
    "    return cost + reg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_gradient_for_loop(X, y, w, b, l):\n",
    "    m, n = X.shape\n",
    "    dw = np.zeros((n,))  # initialize the gradient vector\n",
    "    db = 0.              # initialize the intercept gradient\n",
    "    \n",
    "    for i in range(m):\n",
    "        z = np.dot(X[i], w) + b\n",
    "        a = sigmoid(z)\n",
    "        dz = a - y[i]\n",
    "        for j in range(n):\n",
    "            dw[j] += X[i][j] * dz + l/m * w[j]\n",
    "        db += dz\n",
    "    return dw / m, db / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "\n",
    "def logistic_model(X, y, w_initial, b_initial, learning_rate=0.01, num_iterations=1000, l=0.0):\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_initial)\n",
    "    b = b_initial\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Calculate the predicted values\n",
    "        dw, db = calculate_gradient_for_loop(X, y, w, b, l)\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "    \n",
    "        for j in range(n):\n",
    "            pass\n",
    "        \n",
    "            # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b, l) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iterations / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}\")    \n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "w_tmp  = np.zeros_like(train_X[0])\n",
    "b_tmp  = 0.\n",
    "alpha = 0.1\n",
    "iters = 1000\n",
    "l = 0.7\n",
    "\n",
    "w_out, b_out, history = logistic_model(train_X, train_y, w_tmp, b_tmp, alpha, iters, l) \n",
    "\n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assume that `model` is your trained logistic regression model\n",
    "# and `X_train` and `y_train` are your training data and labels\n",
    "\n",
    "# Compute the model's predictions on the training data\n",
    "# y_pred = model.predict(test_X)\n",
    "z = np.dot(test_X, w_out) + b_out\n",
    "predictions = sigmoid(z)\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(test_y, binary_predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y, ax, pos_label=\"y=1\", neg_label=\"y=0\", s=80, loc='best' ):\n",
    "    \"\"\" plots logistic data with two axis \"\"\"\n",
    "    # Find Indices of Positive and Negative Examples\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    pos = pos.reshape(-1,)  #work with 1D or 1D y vectors\n",
    "    neg = neg.reshape(-1,)\n",
    "\n",
    "    # Plot examples\n",
    "    ax.scatter(X[pos, 0], X[pos, 1], marker='x', s=s, c = 'red', label=pos_label)\n",
    "    ax.scatter(X[neg, 0], X[neg, 1], marker='o', s=s, label=neg_label, facecolors='none', edgecolors=dlblue, lw=3)\n",
    "    ax.legend(loc=loc)\n",
    "\n",
    "    ax.figure.canvas.toolbar_visible = False\n",
    "    ax.figure.canvas.header_visible = False\n",
    "    ax.figure.canvas.footer_visible = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from plt_quad_logistic import plt_quad_logistic, plt_prob\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "print(w_out.shape)\n",
    "w_out = w_out.reshape(-1, 1)\n",
    "#print(w_out.shape)\n",
    "\n",
    "w_pca = pca.fit_transform(w_out)\n",
    "w_pca_flat = w_pca.flatten()\n",
    "\n",
    "print(w_pca.shape)\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,4))\n",
    "# plot the probability \n",
    "\n",
    "plt_prob(ax, w_pca, b_out)\n",
    "\n",
    "# Plot the original data\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.set_xlabel(r'$x_0$')   \n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "plot_data(train_X,train_y,ax)\n",
    "\n",
    "# Plot the decision boundary\n",
    "x0 = -b_out/w_out[0]\n",
    "x1 = -b_out/w_out[1]\n",
    "ax.plot([0,x0],[x1,0], c=dlc[\"dlblue\"], lw=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assume X is your input data and y are your labels\n",
    "X = np.random.rand(100, 2)\n",
    "y = np.random.randint(2, size=100)\n",
    "\n",
    "# Fit a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "\n",
    "# Generate a grid over the input space to plot the color of the\n",
    "#  classification at that grid point\n",
    "xx, yy = np.mgrid[min(X[:,0])-1:max(X[:,0])+1:.01, min(X[:,1])-1:max(X[:,1])+1:.01]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = model.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contour(xx, yy, probs, levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
