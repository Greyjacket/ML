{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asch7\\OneDrive\\Desktop\\ML\\ML_env\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data  # features\n",
    "y = data.target  # target labels (0 for malignant, 1 for benign)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model (multinomial logistic regression)\n",
    "model = LogisticRegression(multi_class='ovr', solver='sag', max_iter=1000)\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn's `LogisticRegression` function provides several solvers for optimization. Here's a brief summary of each:\n",
    "\n",
    "1. **`'liblinear'`**: A Library for Large Linear Classification. It uses a coordinate descent (CD) algorithm, and it can handle both L1 and L2 regularization. It's a good choice for small datasets and is the only solver that supports the \"one versus rest\" scheme when the `multi_class` option is set to `'ovr'`.\n",
    "\n",
    "2. **`'newton-cg'`**: Newton's method with conjugate gradient. It's an optimization algorithm that can handle multiclass problems and L2 regularization. It's a good choice for larger datasets, as it converges faster than `'liblinear'` for these.\n",
    "\n",
    "3. **`'lbfgs'`**: Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm. It's an optimization algorithm in the family of quasi-Newton methods that approximates the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm using a limited amount of computer memory. It's a good choice for larger datasets, and it supports multiclass problems and L2 regularization.\n",
    "\n",
    "4. **`'sag'`**: Stochastic Average Gradient descent. It's a variant of gradient descent, and it's faster than other solvers for large datasets. However, it only supports L2 regularization.\n",
    "\n",
    "5. **`'saga'`**: Stochastic Average Gradient descent with Augmented factor. It's a variant of `'sag'` that also supports the non-smooth penalty='l1' option (i.e., L1 regularization). This is therefore the solver of choice for sparse multinomial logistic regression and it's robust to unscaled datasets.\n",
    "\n",
    "Each solver has its strengths and weaknesses, and the best one to use depends on the nature of your data and the specific requirements of your problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
